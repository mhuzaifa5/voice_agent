{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "28c301ba-4e28-4054-9c15-ab4ab16a20c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1 load the documents\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"Resume.pdf\")\n",
    "documents = loader.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d8d4af75-c9aa-47f2-9b91-94a151d0efb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 2 split the documents\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "docs = text_splitter.split_documents(documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8191934-944f-4660-9a9e-e707e3cc84f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "9d5582ab-ea1d-4497-82d1-e369b0c281e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768\n"
     ]
    }
   ],
   "source": [
    "# step 3 generate embeddings\n",
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "\n",
    "model = SentenceTransformerEmbeddings(model_name=\"all-mpnet-base-v2\")  # 768 dims\n",
    "text = [doc.page_content for doc in docs]\n",
    "\n",
    "# Now pass the strings to embed_documents\n",
    "embeddings = model.embed_documents(text)\n",
    "\n",
    "\n",
    "print(len(embeddings[0]))  # 768 dims\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e0c57dd3-30b2-4a81-aa9f-d573225c301c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client ready: True\n"
     ]
    }
   ],
   "source": [
    "# step 4 setup vector db weaviate \n",
    "\n",
    "import weaviate\n",
    "from weaviate.auth import AuthApiKey\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from your db.env file\n",
    "load_dotenv(\"db.env\")\n",
    "\n",
    "weaviate_url = os.getenv(\"WEAVIATE_URL\")\n",
    "weaviate_api_key = os.getenv(\"WEAVIATE_API_KEY\")\n",
    "\n",
    "# Setup authentication using API key\n",
    "auth = AuthApiKey(api_key=weaviate_api_key)\n",
    "\n",
    "# Create the Weaviate client (v4 syntax)\n",
    "client = weaviate.connect_to_weaviate_cloud(\n",
    "    cluster_url=weaviate_url,\n",
    "    auth_credentials=Auth.api_key(weaviate_api_key),\n",
    ")\n",
    "\n",
    "# Check if the client is ready\n",
    "print(\"Client ready:\", client.is_ready())\n",
    "\n",
    "# Close client to clean up resources\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "35c6d7e7-7708-472a-81f5-5d6bf4e28db1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PMLS\\anaconda3\\Lib\\site-packages\\weaviate\\collections\\classes\\config.py:1963: PydanticDeprecatedSince211: Accessing the 'model_fields' attribute on the instance is deprecated. Instead, you should access this attribute from the model class. Deprecated in Pydantic V2.11 to be removed in V3.0.\n",
      "  for cls_field in self.model_fields:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<weaviate.collections.collection.sync.Collection at 0x2cf84cef350>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# step 5 create vector db class \n",
    "from weaviate.classes.config import Property, DataType, Vectorizers\n",
    "\n",
    "client.collections.create(\n",
    "    name=\"Knowledge\",\n",
    "    properties=[\n",
    "        Property(name=\"text\", data_type=DataType.TEXT),\n",
    "    ],\n",
    "    vectorizer_config={\"vectorizer\": Vectorizers.NONE}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "cebf9b5e-3763-4793-b196-027e92bdad84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 6 load the data  \n",
    "collection = client.collections.get(\"Knowledge\")\n",
    "\n",
    "with collection.batch.fixed_size(batch_size=100) as batch:\n",
    "    for text_item, embedding_vector in zip(text, embeddings):\n",
    "        batch.add_object(\n",
    "            properties={\"text\": text_item},\n",
    "            vector=embedding_vector\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "a17eedd1-7de4-4adc-8257-cca28f8b5220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'PROFESSIONAL EXPERIENCE  \\n \\nPython Developer Intern  | Interncraft, Pakistan                                                                         August 2024-October 2024 \\n• Developed a dynamic weather application utilizing the OpenWeatherMap API to deliver accurate, up -to-\\ndate weather data . \\n• Successfully integrated the solution into a comprehensive agriculture tool, significantly improving \\noperational efficiency and decision-making.'}\n"
     ]
    }
   ],
   "source": [
    "#step 7 query the data \n",
    "collection = client.collections.get(\"Knowledge\")\n",
    "\n",
    "response = collection.query.near_vector(\n",
    "    near_vector=embeddings[1],  # your embedding vector, e.g., list of floats\n",
    "    limit=1\n",
    ")\n",
    "\n",
    "for obj in response.objects:\n",
    "    print(obj.properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "e1be973e-8d0a-435e-838f-e24441bf64e3",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Collection' object has no attribute 'schema'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[105], line 10\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mweaviate\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Connect to your Weaviate instance\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \n\u001b[0;32m      8\u001b[0m \n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Create a LangChain vector store from your Weaviate collection\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m db \u001b[38;5;241m=\u001b[39m Weaviate\u001b[38;5;241m.\u001b[39mfrom_documents(docs, embeddings, client\u001b[38;5;241m=\u001b[39mcollection)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Now you can use LangChain's retrieval and generation pipeline\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# For example, using a RetrievalQA chain (assuming you have a suitable LLM chain set up)\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchains\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RetrievalQA\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_core\\vectorstores\\base.py:848\u001b[0m, in \u001b[0;36mVectorStore.from_documents\u001b[1;34m(cls, documents, embedding, **kwargs)\u001b[0m\n\u001b[0;32m    845\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(ids):\n\u001b[0;32m    846\u001b[0m         kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mids\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m ids\n\u001b[1;32m--> 848\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mfrom_texts(texts, embedding, metadatas\u001b[38;5;241m=\u001b[39mmetadatas, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_community\\vectorstores\\weaviate.py:471\u001b[0m, in \u001b[0;36mWeaviate.from_texts\u001b[1;34m(cls, texts, embedding, metadatas, client, weaviate_url, weaviate_api_key, batch_size, index_name, text_key, by_text, relevance_score_fn, **kwargs)\u001b[0m\n\u001b[0;32m    469\u001b[0m schema \u001b[38;5;241m=\u001b[39m _default_schema(index_name, text_key)\n\u001b[0;32m    470\u001b[0m \u001b[38;5;66;03m# check whether the index already exists\u001b[39;00m\n\u001b[1;32m--> 471\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m client\u001b[38;5;241m.\u001b[39mschema\u001b[38;5;241m.\u001b[39mexists(index_name):\n\u001b[0;32m    472\u001b[0m     client\u001b[38;5;241m.\u001b[39mschema\u001b[38;5;241m.\u001b[39mcreate_class(schema)\n\u001b[0;32m    474\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m embedding\u001b[38;5;241m.\u001b[39membed_documents(texts) \u001b[38;5;28;01mif\u001b[39;00m embedding \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Collection' object has no attribute 'schema'"
     ]
    }
   ],
   "source": [
    "# step 8\n",
    "\n",
    "from langchain_community.vectorstores import Weaviate\n",
    "import weaviate\n",
    "\n",
    "# Connect to your Weaviate instance\n",
    "\n",
    "\n",
    "# Create a LangChain vector store from your Weaviate collection\n",
    "db = Weaviate.from_documents(docs, embeddings, client=collection)\n",
    "\n",
    "# Now you can use LangChain's retrieval and generation pipeline\n",
    "# For example, using a RetrievalQA chain (assuming you have a suitable LLM chain set up)\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=model,  # Replace with your LLM, e.g., Google Generative AI if supported\n",
    "    retriever=db.as_retriever()\n",
    ")\n",
    "\n",
    "result = qa_chain.run(\"Your question here\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "3aee9cb6-f7e9-42af-a7e8-d55474fd5e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai  \n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from your db.env file\n",
    "load_dotenv(\"db.env\")\n",
    "google_api_key=os.getenv('GEMINI_API_KEY')\n",
    "genai.configure(api_key=google_api_key) \n",
    "model = genai.GenerativeModel('models/gemini-1.5-flash') \n",
    "response = model.generate_content(\"where was newton born , only give me one or two words response in urdu language only\") \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "4105c022-7a07-490d-93e3-23429b7c796b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "لنکاشائر، انگلینڈ\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(response.text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
